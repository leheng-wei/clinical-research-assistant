import streamlit as st
import fitz
import hashlib
import requests
import os
from pptx import Presentation
from pptx.util import Pt, Inches
from pptx.enum.text import PP_ALIGN
from pptx.dml.color import RGBColor
from docx import Document
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from io import BytesIO
from datetime import datetime, timedelta
from tenacity import retry, stop_after_attempt, wait_exponential
import pandas as pd
import base64
import json
import uuid
import io
import csv
import re

# ===== é…ç½® =====
# 
# ===== é…ç½® =====
DEEPSEEK_API_KEY = st.secrets.get("DEEPSEEK_API_KEY", "")
if not DEEPSEEK_API_KEY:
    st.error("è¯·åœ¨ .streamlit/secrets.toml ä¸­è®¾ç½® DEEPSEEK_API_KEY")

# å¸¸é‡å®šä¹‰
MAX_FILE_SIZE = 200 * 1024 * 1024  # 200MB
MAX_FILES_PER_BATCH = 5
UPLOAD_DIR = "uploaded_pdfs"
LOGO_PATH = "bofu_logo.png"  # ç§»åŠ¨åˆ°é…ç½®éƒ¨åˆ†
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ===== æ–‡ä»¶å¤„ç†å‡½æ•° =====
def validate_file(uploaded_file):
    """éªŒè¯ä¸Šä¼ æ–‡ä»¶çš„å¤§å°å’Œç±»å‹"""
    if len(uploaded_file.getvalue()) > MAX_FILE_SIZE:
        st.error(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆ200MBï¼‰ï¼š{uploaded_file.name}")
        return False
    if not uploaded_file.name.lower().endswith('.pdf'):
        st.error(f"åªæ”¯æŒPDFæ–‡ä»¶ï¼š{uploaded_file.name}")
        return False
    return True

def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å®‰å…¨çš„å­—ç¬¦"""
    safe_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-._")
    filename = ''.join(c for c in filename if c in safe_chars)
    return filename or "unnamed_file"

def process_large_file(uploaded_file):
    """åˆ†å—å¤„ç†å¤§æ–‡ä»¶ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    chunk_size = 1024 * 1024  # 1MB chunks
    file_bytes = uploaded_file.getvalue()
    total_chunks = (len(file_bytes) + chunk_size - 1) // chunk_size
    
    progress_bar = st.progress(0)
    text_blocks = []
    
    for i in range(0, len(file_bytes), chunk_size):
        chunk = file_bytes[i:i + chunk_size]
        doc = fitz.open(stream=chunk, filetype="pdf")
        
        for page in doc:
            blocks = page.get_text("blocks")
            for b in blocks:
                if b[6] == 0:
                    clean_text = b[4].replace('\x00', '').strip()
                    if clean_text and len(clean_text) > 20:
                        text_blocks.append(clean_text)
        
        progress = (i + chunk_size) / len(file_bytes)
        progress_bar.progress(min(progress, 1.0))
    
    return "\n".join(text_blocks)

def process_file_with_status(uploaded_file):
    """æ·»åŠ å¤„ç†çŠ¶æ€æç¤ºçš„æ–‡ä»¶å¤„ç†å‡½æ•°"""
    status_placeholder = st.empty()
    progress_placeholder = st.empty()
    progress_bar = progress_placeholder.progress(0)
    
    try:
        # æ­¥éª¤1ï¼šéªŒè¯æ–‡ä»¶
        status_placeholder.info("ğŸ“‘ éªŒè¯æ–‡ä»¶...")
        if not validate_file(uploaded_file):
            status_placeholder.error("âŒ æ–‡ä»¶éªŒè¯å¤±è´¥")
            return None
        progress_bar.progress(25)
        
        # æ­¥éª¤2ï¼šæå–æ–‡æœ¬
        status_placeholder.info("ğŸ“– æå–æ–‡æœ¬...")
        file_bytes = uploaded_file.getvalue()
        file_hash = hashlib.md5(file_bytes).hexdigest()
        full_text = extract_text_from_pdf_by_hash(file_hash, file_bytes)
        if not full_text:
            status_placeholder.error("âŒ æ–‡æœ¬æå–å¤±è´¥")
            return None
        progress_bar.progress(50)
        
        # æ­¥éª¤3ï¼šåˆ†æå†…å®¹
        status_placeholder.info("ğŸ¤– åˆ†æå†…å®¹...")
        trimmed_text = full_text[:30000]
        text_hash = hashlib.md5(trimmed_text.encode("utf-8")).hexdigest()
        result = extract_study_by_hash(text_hash, trimmed_text)
        if not result:
            status_placeholder.error("âŒ å†…å®¹åˆ†æå¤±è´¥")
            return None
        progress_bar.progress(75)
        
        # æ­¥éª¤4ï¼šç”ŸæˆæŠ¥å‘Š
        status_placeholder.info("ğŸ“Š ç”ŸæˆæŠ¥å‘Š...")
        try:
            # ç”ŸæˆCSV
            csv_lines = []
            for line in result.splitlines():
                if "|" in line and not line.startswith("|---") and not line.lower().startswith("| è¦ç´ "):
                    parts = [p.strip() for p in line.strip("|").split("|")]
                    if len(parts) >= 2:
                        csv_lines.append(",".join(parts))
            csv_lines.insert(0, "è¦ç´ ,å†…å®¹")
            
            # ç”ŸæˆPPT
            prs = Presentation()
            prs.slide_width = Inches(13.33)
            prs.slide_height = Inches(7.5)
            
            # ç”ŸæˆWord
            word_bytes = generate_word_table(csv_lines, "åšæ‰¶AIåˆ›æ„ç»„", uploaded_file.name)
            
            progress_bar.progress(100)
            status_placeholder.success("âœ… å¤„ç†å®Œæˆ")
            return result, csv_lines, prs, word_bytes
            
        except Exception as e:
            status_placeholder.error("âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            return None
            
    except Exception as e:
        status_placeholder.error("âŒ å¤„ç†å¤±è´¥")
        return None
    finally:
        # æ¸…ç†çŠ¶æ€æ˜¾ç¤º
        progress_placeholder.empty()
        status_placeholder.empty()

# ===== æ–‡æœ¬æå–å‡½æ•° =====
@st.cache_data(show_spinner="ğŸ“– æ­£åœ¨è§£æ PDF", max_entries=100)
def extract_text_from_pdf_by_hash(file_hash: str, file_bytes: bytes):
    """æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©å¤„ç†æ–¹å¼"""
    if len(file_bytes) > MAX_FILE_SIZE:
        return process_large_file(file_bytes)
    else:
        doc = fitz.open(stream=file_bytes, filetype="pdf")
        text_blocks = []
        for page in doc:
            blocks = page.get_text("blocks")
            for b in blocks:
                if b[6] == 0:
                    clean_text = b[4].replace('\x00', '').strip()
                    if clean_text and len(clean_text) > 20:
                        text_blocks.append(clean_text)
        return "\n".join(text_blocks)

@st.cache_data(show_spinner="ğŸ¤– æ­£åœ¨è°ƒç”¨æ¨¡å‹...", max_entries=100)
def extract_study_by_hash(text_hash: str, text: str):
    return extract_study_design(text)

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def extract_study_design(text):
    """æå–ç ”ç©¶è®¾è®¡ä¿¡æ¯ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    prompt = f"""ä½œä¸ºä¸´åºŠç ”ç©¶ä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ç ”ç©¶è®¾è®¡ä¿¡æ¯ï¼Œå¹¶è¾“å‡ºä¸º Markdown è¡¨æ ¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| è¦ç´  | å†…å®¹ |
|------|------|
| ç ”ç©¶ç±»å‹ | RCT / é˜Ÿåˆ— / ç—…ä¾‹å¯¹ç…§ / æ¨ªæ–­é¢ç­‰ |
| æ˜¯å¦å¤šä¸­å¿ƒ | æ˜¯/å¦ |
| æ˜¯å¦ç›²æ³• | å•ç›² / åŒç›² / å¼€æ”¾æ ‡ç­¾ |
| çº³å…¥/æ’é™¤æ ‡å‡† | ç®€è¦åˆ—å‡º |
| å¹²é¢„æªæ–½ï¼ˆå®éªŒç»„ï¼‰ | è¯ç‰©ã€å‰‚é‡ã€é¢‘ç‡ç­‰ |
| å¹²é¢„æªæ–½ï¼ˆå¯¹ç…§ç»„ï¼‰ | å¦‚å®‰æ…°å‰‚/æ ‡å‡†æ²»ç–—ç­‰ |
| æ‚£è€…äººæ•° | æ ·æœ¬æ€»é‡åŠåˆ†ç»„æ•°é‡ |
| ä¸»è¦ç»ˆç‚¹æŒ‡æ ‡ | ç–—æ•ˆæŒ‡æ ‡åç§°ä¸è¯„ä¼°æ–¹å¼ |
| æ¬¡è¦/å…¶ä»–ç»ˆç‚¹æŒ‡æ ‡ | åŒ…æ‹¬æ‰€æœ‰éä¸»è¦ç»ˆç‚¹çš„ç–—æ•ˆè§‚å¯ŸæŒ‡æ ‡ï¼Œå¦‚ secondary outcomesã€exploratory outcomesã€å…¶ä»–æ•ˆæœè¯„ä¼°ç­‰ |
| å…³é”®é‡åŒ–æŒ‡æ ‡ | æ‰€æœ‰è¢«ç”¨äºé‡åŒ–åˆ†æã€æœºåˆ¶ç ”ç©¶ã€å»ºæ¨¡æˆ–åˆ†ç»„çš„é‡è¦å˜é‡ã€‚ä¸é™ç»ˆç‚¹ã€ä¸é™ç–—æ•ˆç›¸å…³ï¼Œå¦‚ insulin resistance, clearance ç­‰ |
| å®‰å…¨æ€§ç»ˆç‚¹æŒ‡æ ‡ | ä¸è‰¯äº‹ä»¶ã€å®éªŒå®¤æŒ‡æ ‡ç­‰ |
| ç»Ÿè®¡åˆ†ææ–¹æ³• | æ‰€ç”¨ç»Ÿè®¡å·¥å…·å’Œæ¨¡å‹ |
| ä¸´åºŠè¯•éªŒæ³¨å†Œå· | å¦‚æœ‰è¯·åˆ—å‡º |

æ–‡çŒ®å†…å®¹å¦‚ä¸‹ï¼š
{text}
"""
    try:
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {DEEPSEEK_API_KEY}"},
            json={
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0,
                "max_tokens": 4000
            },
            timeout=90
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        st.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ˆå°†è‡ªåŠ¨é‡è¯•ï¼‰ï¼š{e}")
        raise
    except Exception as e:
        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return ""

def extract_supplementary_notes(result):
    """æå–è¡¥å……è¯´æ˜å†…å®¹"""
    if not isinstance(result, str):
        return None
    
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹å¼æ£€æµ‹è¡¥å……è¯´æ˜
    supplementary_pattern = r"è¡¥å……è¯´æ˜[ï¼š:]\s*(.*?)(?=\n\n|\Z)"
    match = re.search(supplementary_pattern, result, re.DOTALL)
    
    if match:
        return match.group(1).strip()
    return None

# ===== æ–‡æ¡£ç”Ÿæˆå‡½æ•° =====
def generate_word_table(csv_lines, team_name, source_file):
    doc = Document()
    heading = doc.add_heading(f"{team_name} Â· ä¸´åºŠç ”ç©¶ç»“æ„åŒ–æå–æŠ¥å‘Š", 0)
    heading.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
    subtitle = doc.add_paragraph(f"ğŸ“„ æ¥æºæ–‡çŒ®ï¼š{source_file}", style="Intense Quote")
    subtitle.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
    doc.add_paragraph()
    table = doc.add_table(rows=0, cols=2)
    table.style = 'Table Grid'
    for row in csv_lines[1:]:
        if "," in row:
            key, value = row.split(",", 1)
            cells = table.add_row().cells
            cells[0].text = key.strip()
            cells[1].text = value.strip()
    for row in table.rows:
        for cell in row.cells:
            for para in cell.paragraphs:
                para.paragraph_format.line_spacing = 1.5
                para.paragraph_format.space_after = Pt(6)
                para.paragraph_format.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT
                para.runs[0].font.size = Pt(11)
    doc.add_paragraph(f"å¯¼å‡ºæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    word_stream = BytesIO()
    doc.save(word_stream)
    word_stream.seek(0)
    return word_stream

# ===== å†å²è®°å½•ç®¡ç†å‡½æ•° =====
def load_history():
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å†å²è®°å½•"""
    history_file = 'history.json'
    if not os.path.exists(history_file):
        print("å†å²è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„è®°å½•æ–‡ä»¶")
        return []
    
    try:
        with open(history_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if not isinstance(data, dict) or 'records' not in data:
                print("å†å²è®°å½•æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†é‡ç½®è®°å½•")
                return []
            records = data.get('records', [])
            if not isinstance(records, list):
                print("å†å²è®°å½•æ ¼å¼é”™è¯¯ï¼Œå°†é‡ç½®è®°å½•")
                return []
            
            # æ¸…ç†è¶…è¿‡1å‘¨çš„è®°å½•
            current_time = datetime.now()
            one_week_ago = current_time - timedelta(days=7)
            cleaned_records = [
                record for record in records 
                if datetime.strptime(record['æ—¶é—´'], '%Y-%m-%d %H:%M:%S') > one_week_ago
            ]
            return cleaned_records
    except Exception as e:
        print(f"åŠ è½½å†å²è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def save_history(records):
    """ä¿å­˜å†å²è®°å½•åˆ°æœ¬åœ°æ–‡ä»¶"""
    if not isinstance(records, list):
        print("è®°å½•æ ¼å¼é”™è¯¯ï¼Œå–æ¶ˆä¿å­˜")
        return False
    
    try:
        # æ¸…ç†è¶…è¿‡1å‘¨çš„è®°å½•
        current_time = datetime.now()
        one_week_ago = current_time - timedelta(days=7)
        cleaned_records = [
            record for record in records 
            if datetime.strptime(record['æ—¶é—´'], '%Y-%m-%d %H:%M:%S') > one_week_ago
        ]
        
        data = {
            "records": cleaned_records,
            "metadata": {
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_records": len(cleaned_records),
                "cleaned_records": len(records) - len(cleaned_records)
            }
        }
        with open('history.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜å†å²è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def delete_record(record_id):
    """åˆ é™¤å•æ¡å†å²è®°å½•"""
    if not record_id:
        print("è®°å½•IDæ— æ•ˆ")
        return False
        
    if 'history' not in st.session_state:
        print("å†å²è®°å½•çŠ¶æ€æœªåˆå§‹åŒ–")
        return False
        
    try:
        records = st.session_state.history
        original_length = len(records)
        st.session_state.history = [r for r in records if r.get('id') != record_id]
        
        if len(st.session_state.history) == original_length:
            print(f"æœªæ‰¾åˆ°IDä¸º {record_id} çš„è®°å½•")
            return False
            
        if save_history(st.session_state.history):
            print(f"æˆåŠŸåˆ é™¤è®°å½• {record_id}")
            return True
        else:
            print("åˆ é™¤è®°å½•åä¿å­˜å¤±è´¥")
            return False
    except Exception as e:
        print(f"åˆ é™¤è®°å½•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

# ===== é¡µé¢å±•ç¤ºå‡½æ•° =====
def show_history():
    st.subheader("ğŸ“œ å¤„ç†å†å²è®°å½•")
    if len(st.session_state.history) > 0:
        history_df = pd.DataFrame(st.session_state.history)
        history_df['æ—¶é—´'] = pd.to_datetime(history_df['æ—¶é—´'])
        history_df.sort_values('æ—¶é—´', ascending=False, inplace=True)

        # å±•ç¤ºå†å²è®°å½•è¡¨æ ¼
        st.dataframe(history_df)

        # æä¾›ä¸‹è½½å†å²è®°å½•åŠŸèƒ½
        csv = history_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ä¸‹è½½å†å²è®°å½• CSV",
            data=csv,
            file_name="å†å²è®°å½•.csv",
            mime="text/csv"
        )

        # æä¾›æ¸…é™¤å†å²è®°å½•æŒ‰é’®
        if st.button("æ¸…é™¤æ‰€æœ‰å†å²è®°å½•"):
            st.session_state.history.clear()
            st.success("å†å²è®°å½•å·²æ¸…é™¤ï¼")
    else:
        st.warning("æ²¡æœ‰å†å²è®°å½•ã€‚")

def clear_uploaded_files():
    """æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    if "uploaded_pdfs" in os.listdir():
        files = os.listdir("uploaded_pdfs")
        if len(files) > 0:
            for file in files:
                os.remove(os.path.join("uploaded_pdfs", file))
            st.success("ä¸Šä¼ æ–‡ä»¶å·²æ¸…ç†ï¼")
        else:
            st.warning("æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶å¯æ¸…ç†ï¼")
    else:
        st.warning("æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨ï¼")

def admin_dashboard():
    st.title("åå°ç®¡ç† - ä¸´åºŠç ”ç©¶è®¾è®¡ç»“æ„åŒ–åŠ©æ‰‹")
    
    # è·å–å†å²è®°å½•ï¼Œè¯»å–æœ¬åœ°æ–‡ä»¶æˆ–è€…ç¼“å­˜æ•°æ®
    if 'history' not in st.session_state:
        st.session_state.history = []
    
    action = st.selectbox("é€‰æ‹©æ“ä½œ", ["æŸ¥çœ‹å†å²è®°å½•", "æ¸…ç†ä¸Šä¼ æ–‡ä»¶", "é€€å‡ºåå°"])

    if action == "æŸ¥çœ‹å†å²è®°å½•":
        show_history()
    elif action == "æ¸…ç†ä¸Šä¼ æ–‡ä»¶":
        clear_uploaded_files()
    else:
        st.write("é€€å‡ºåå°ç®¡ç†ç•Œé¢")

# ===== ä¸»ç¨‹åº =====
st.set_page_config(layout="wide", page_title="ä¸´åºŠç ”ç©¶è®¾è®¡ç»“æ„åŒ–åŠ©æ‰‹")
page = st.radio("é€‰æ‹©é¡µé¢", ["ä¸»é¡µ", "åå°ç®¡ç†"], horizontal=True)

# åˆå§‹åŒ–æˆ–åŠ è½½å†å²è®°å½•
if 'history' not in st.session_state:
    st.session_state.history = load_history()

if page == "ä¸»é¡µ":
    # ä¸Šä¼ æ–‡çŒ®å¹¶å±•ç¤ºç»“æ„åŒ–æå–ç»“æœ
    st.markdown('<div style="display: flex; align-items: flex-start; padding-top: 12px; font-size: 45px;">ğŸ¤–</div>', unsafe_allow_html=True)
    st.markdown('<h1 style="margin: 0;">ä¸´åºŠç ”ç©¶è®¾è®¡ç»“æ„åŒ–åŠ©æ‰‹</h1>', unsafe_allow_html=True)
    st.caption("ğŸ’¡ è‡ªåŠ¨è¯†åˆ« PDF æ–‡çŒ®ä¸­çš„ç ”ç©¶è®¾è®¡ä¿¡æ¯ï¼Œæ”¯æŒå¯¼å‡ºä¸º CSV / PPT / Word")
    
    # æ·»åŠ å¤„ç†çŠ¶æ€è¯´æ˜
    with st.expander("ğŸ’¡ å¤„ç†è¯´æ˜", expanded=False):
        st.info("""
        **å¤„ç†æ­¥éª¤ï¼š**
        1. ğŸ“‘ éªŒè¯æ–‡ä»¶
        2. ğŸ“– æå–æ–‡æœ¬
        3. ğŸ¤– åˆ†æå†…å®¹
        4. ğŸ“Š ç”ŸæˆæŠ¥å‘Š
        
        **æç¤ºï¼š**
        - æ¯ç¯‡æ–‡çŒ®å¤„ç†æ—¶é—´çº¦1-2åˆ†é’Ÿ
        - å¤„ç†è¿‡ç¨‹ä¸­è¯·å‹¿å…³é—­é¡µé¢
        """)
    
    uploaded_files = st.file_uploader("ğŸ“„ ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼Œæ¯ä¸ªæ–‡ä»¶é™åˆ¶200MBï¼‰", type=["pdf"], accept_multiple_files=True)
    total_files = len(uploaded_files) if uploaded_files else 0
    if uploaded_files:
       total_files = len(uploaded_files)
    if total_files > 5:
        st.error(f"âŒ è¶…å‡ºå•æ¬¡å¤„ç†é™åˆ¶ï¼ˆ5ç¯‡ï¼‰")
        # åªå¤„ç†å‰5ç¯‡
        current_batch = uploaded_files[:5]
        queued_files = uploaded_files[5:]
    else:
        current_batch = uploaded_files
        queued_files = []
    if current_batch:
        tabs = st.tabs([f"ğŸ“„ {i+1}. {file.name}" for i, file in enumerate(current_batch)])
        for idx, (tab, uploaded_file) in enumerate(zip(tabs, current_batch)):
            with tab:
            # å¤„ç†é€»è¾‘
              ...
    else:
        st.info("è¯·ä¸Šä¼ ä¸€ç¯‡æˆ–å¤šç¯‡ PDF æ–‡çŒ®ä»¥å¼€å§‹å¤„ç†ã€‚")

    st.caption(f"ğŸ“š å½“å‰å¤„ç†ï¼š{len(current_batch)} ç¯‡æ–‡çŒ®" + (f" | é˜Ÿåˆ—ä¸­ï¼š{len(queued_files)} ç¯‡" if queued_files else ""))
    
    # å¤„ç†ä¸Šä¼ çš„æ–‡çŒ®
    tabs = st.tabs([f"ğŸ“„ {i+1}. {file.name}" for i, file in enumerate(current_batch)])

    for idx, (tab, uploaded_file) in enumerate(zip(tabs, current_batch)):
        with tab:
            # è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼
            file_bytes = uploaded_file.read()
            uploaded_file.seek(0)  # å°†æ–‡ä»¶æŒ‡é’ˆé‡ç½®åˆ°æ–‡ä»¶å¼€å¤´ï¼Œä»¥ä¾¿åç»­å¤„ç†
            file_hash = hashlib.sha256(file_bytes).hexdigest()

            # ä½¿ç”¨æ–°çš„å¤„ç†å‡½æ•°
            result = process_file_with_status(uploaded_file)
            
            if not result:
                st.error(f"âŒ å¤„ç†å¤±è´¥ï¼š{uploaded_file.name}")
                continue

            result, csv_lines, prs, word_bytes = result
            
            # ç»“æ„åŒ–è½¬ CSV è¡Œ
            today = datetime.now().strftime("%Y%m%d")
            csv_bytes = BytesIO("\n".join(csv_lines).encode("utf-8"))
            csv_bytes.seek(0)

            # ===== PPT ç”Ÿæˆï¼ˆå¸¦ LOGO å°é¢ï¼‰ =====
            prs = Presentation()
            prs.slide_width = Inches(13.33)
            prs.slide_height = Inches(7.5)
            
            # å°é¢é¡µ
            cover = prs.slides.add_slide(prs.slide_layouts[6])
            title_box = cover.shapes.add_textbox(Inches(1), Inches(0.8), Inches(11), Inches(1.5))
            tf = title_box.text_frame
            p = tf.paragraphs[0]
            run = p.add_run()
            run.text = "ç ”ç©¶è®¾è®¡æå–æŠ¥å‘Š"
            run.font.size = Pt(42)
            run.font.name = "å¾®è½¯é›…é»‘"
            run.font.bold = True
            run.font.color.rgb = RGBColor(0, 51, 102)
            p.alignment = PP_ALIGN.CENTER
            
            if os.path.exists(LOGO_PATH):
                cover.shapes.add_picture(LOGO_PATH, Inches(4.8), Inches(2.0), height=Inches(0.6))
            
            sub_box = cover.shapes.add_textbox(Inches(1), Inches(4.0), Inches(11), Inches(0.8))
            tf2 = sub_box.text_frame
            tf2.text = f"æºæ–‡ä»¶ï¼š  {uploaded_file.name}"
            tf2.paragraphs[0].font.size = Pt(20)
            tf2.paragraphs[0].font.name = "å¾®è½¯é›…é»‘"
            tf2.paragraphs[0].alignment = PP_ALIGN.CENTER
            
            # å†…å®¹é¡µ
            for i, row in enumerate(csv_lines[1:]):
                parts = row.split(",", 1)
                if len(parts) == 2:
                    slide = prs.slides.add_slide(prs.slide_layouts[1])
                    title_shape = slide.shapes.title
                    title_shape.text = parts[0]
                    title_frame = title_shape.text_frame
                    title_frame.paragraphs[0].alignment = PP_ALIGN.LEFT
                    title_run = title_frame.paragraphs[0].runs[0]
                    title_run.font.name = "å¾®è½¯é›…é»‘"
                    title_run.font.bold = True
                    title_run.font.color.rgb = RGBColor(0, 51, 102)
                    
                    textbox = slide.placeholders[1]
                    textbox.text = parts[1].replace("ï¼›", "\n")
                    for p in textbox.text_frame.paragraphs:
                        for run in p.runs:
                            run.font.size = Pt(18)
                            run.font.name = "å¾®è½¯é›…é»‘"
                    
                    footer = slide.shapes.add_textbox(Inches(0.5), Inches(6.9), Inches(12), Inches(0.5))
                    tf_footer = footer.text_frame
                    tf_footer.text = f"åšæ‰¶AIåˆ›æ„ç»„ Â· ç»“æ„åŒ–åŠ©æ‰‹ Â· ç¬¬ {i+1} é¡µ"
                    tf_footer.paragraphs[0].font.size = Pt(10)
                    tf_footer.paragraphs[0].font.name = "å¾®è½¯é›…é»‘"
                    tf_footer.paragraphs[0].alignment = PP_ALIGN.RIGHT
            
            # æ·»åŠ è¡¥å……è¯´æ˜é¡µ
            if "è¡¥å……è¯´æ˜" in result:
                slide = prs.slides.add_slide(prs.slide_layouts[1])
                title_shape = slide.shapes.title
                title_shape.text = "è¡¥å……è¯´æ˜"
                title_frame = title_shape.text_frame
                title_frame.paragraphs[0].alignment = PP_ALIGN.LEFT
                title_run = title_frame.paragraphs[0].runs[0]
                title_run.font.name = "å¾®è½¯é›…é»‘"
                title_run.font.bold = True
                title_run.font.color.rgb = RGBColor(0, 51, 102)
                
                textbox = slide.placeholders[1]
                textbox.text = result.split("è¡¥å……è¯´æ˜ï¼š")[1].strip()
                for p in textbox.text_frame.paragraphs:
                    for run in p.runs:
                        run.font.size = Pt(18)
                        run.font.name = "å¾®è½¯é›…é»‘"
                
                footer = slide.shapes.add_textbox(Inches(0.5), Inches(6.9), Inches(12), Inches(0.5))
                tf_footer = footer.text_frame
                tf_footer.text = f"åšæ‰¶AIåˆ›æ„ç»„ Â· ç»“æ„åŒ–åŠ©æ‰‹ Â· è¡¥å……è¯´æ˜é¡µ"
                tf_footer.paragraphs[0].font.size = Pt(10)
                tf_footer.paragraphs[0].font.name = "å¾®è½¯é›…é»‘"
                tf_footer.paragraphs[0].alignment = PP_ALIGN.RIGHT
            
            pptx_bytes = BytesIO()
            prs.save(pptx_bytes)
            pptx_bytes.seek(0)

            # ===== å±•ç¤ºç»“æœ =====
            st.success("âœ… å·²æˆåŠŸæå–ç»“æ„åŒ–ç ”ç©¶è®¾è®¡ä¿¡æ¯")
            st.markdown(result.strip(), unsafe_allow_html=True)
            
            # æ·»åŠ æç¤ºä¿¡æ¯
            st.info("ğŸ’¡ æç¤ºï¼šè¯·åŠæ—¶ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶ï¼Œå†å²è®°å½•å°†åœ¨7å¤©åè‡ªåŠ¨æ¸…ç†ã€‚")
            st.warning("âš ï¸ æ³¨æ„ï¼šæ–‡ä»¶ä»…ä¿å­˜åœ¨æµè§ˆå™¨ä¼šè¯ä¸­ï¼Œå…³é—­é¡µé¢åå°†æ— æ³•è®¿é—®ã€‚")

            # ç”Ÿæˆæ–‡ä»¶å
            today = datetime.now().strftime("%Y%m%d")
            base_name = os.path.splitext(uploaded_file.name)[0]

            st.markdown("#### ğŸ“ ä¸‹è½½å¯¼å‡ºæ–‡ä»¶")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.download_button("ğŸ“¥ ä¸‹è½½ CSV", csv_bytes, f"{today}_{base_name}_ç»“æ„åŒ–.csv", mime="text/csv")
            with col2:
                st.download_button("ğŸ“Š ä¸‹è½½ PPT", pptx_bytes, f"{today}_{base_name}_ç»“æ„åŒ–.pptx", mime="application/vnd.openxmlformats-officedocument.presentationml.presentation")
            with col3:
                st.download_button("ğŸ“„ ä¸‹è½½ Word", word_bytes, f"{today}_{base_name}_ç»“æ„åŒ–.docx", mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

            # ä¿å­˜è®°å½•åˆ°å†å²
            record = {
                "id": str(uuid.uuid4()),
                "æ–‡ä»¶å": uploaded_file.name,
                "hash": file_hash,
                "æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "æå–å†…å®¹": result.strip(),
                "æ–‡ä»¶æ•°æ®": {
                    "CSV": base64.b64encode(csv_bytes.getvalue()).decode('utf-8'),
                    "PPT": base64.b64encode(pptx_bytes.getvalue()).decode('utf-8'),
                    "Word": base64.b64encode(word_bytes.getvalue()).decode('utf-8')
                }
            }
            st.session_state.history.append(record)
            save_history(st.session_state.history)

            st.markdown("---")
            st.subheader("ğŸ“œ å†å²å¤„ç†è®°å½•")
            for record in st.session_state.history:
                with st.expander(f"ğŸ“„ `{record['æ–‡ä»¶å']}`"):
                    col1, col2 = st.columns([0.9, 0.1])
                    with col1:
                        st.markdown(f"ğŸ“ æ–‡ä»¶ Hash: `{record['hash']}`")
                        st.markdown(f"â° æ—¶é—´: {record['æ—¶é—´']}")
                        st.markdown(f"ğŸ“„ æå–å†…å®¹:\n{record['æå–å†…å®¹']}")

                        # ä»base64å­—ç¬¦ä¸²æ¢å¤æ•°æ®å¹¶æä¾›ä¸‹è½½
                        st.markdown(f"#### ä¸‹è½½æ–‡ä»¶:")
                        download_col1, download_col2, download_col3 = st.columns(3)
                        with download_col1:
                            try:
                                csv_data = base64.b64decode(record['æ–‡ä»¶æ•°æ®']['CSV'])
                                st.download_button("ğŸ“¥ ä¸‹è½½ CSV", 
                                                data=csv_data, 
                                                file_name=f"{record['æ–‡ä»¶å']}_ç»“æ„åŒ–.csv",
                                                mime="text/csv",
                                                key=f"csv_{record['id']}")
                            except Exception as e:
                                st.error(f"CSVæ•°æ®åŠ è½½å¤±è´¥")
                        
                        with download_col2:
                            try:
                                pptx_data = base64.b64decode(record['æ–‡ä»¶æ•°æ®']['PPT'])
                                st.download_button("ğŸ“Š ä¸‹è½½ PPT", 
                                                data=pptx_data,
                                                file_name=f"{record['æ–‡ä»¶å']}_ç»“æ„åŒ–.pptx",
                                                mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                                                key=f"ppt_{record['id']}")
                            except Exception as e:
                                st.error(f"PPTæ•°æ®åŠ è½½å¤±è´¥")
                        
                        with download_col3:
                            try:
                                word_data = base64.b64decode(record['æ–‡ä»¶æ•°æ®']['Word'])
                                st.download_button("ğŸ“„ ä¸‹è½½ Word",
                                                data=word_data,
                                                file_name=f"{record['æ–‡ä»¶å']}_ç»“æ„åŒ–.docx",
                                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                                key=f"word_{record['id']}")
                            except Exception as e:
                                st.error(f"Wordæ•°æ®åŠ è½½å¤±è´¥")
                        
                    with col2:
                        if st.button("ğŸ—‘ï¸", key=f"delete_{record['id']}", help="åˆ é™¤æ­¤è®°å½•"):
                            delete_record(record['id'])
                            st.rerun()

elif page == "åå°ç®¡ç†":
    admin_dashboard()

st.markdown("---")
st.markdown("Â© 2025 åšæ‰¶AIåˆ›æ„ç»„ Â· åŒ»å­¦æ–‡çŒ®ç»“æ„åŒ–åŠ©æ‰‹")
